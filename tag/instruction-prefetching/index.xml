<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>instruction prefetching | David Schall</title><link>https://dhschall.github.io/tag/instruction-prefetching/</link><atom:link href="https://dhschall.github.io/tag/instruction-prefetching/index.xml" rel="self" type="application/rss+xml"/><description>instruction prefetching</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 22 Jun 2022 13:50:00 +0000</lastBuildDate><image><url>https://dhschall.github.io/media/icon_hu981027b910824d70bbcbf5f2f23e00a3_21003_512x512_fill_lanczos_center_3.png</url><title>instruction prefetching</title><link>https://dhschall.github.io/tag/instruction-prefetching/</link></image><item><title>Lukewarm serverless functions: characterization and optimization</title><link>https://dhschall.github.io/talk/lukewarm-serverless-functions-characterization-and-optimization/</link><pubDate>Wed, 22 Jun 2022 13:50:00 +0000</pubDate><guid>https://dhschall.github.io/talk/lukewarm-serverless-functions-characterization-and-optimization/</guid><description>&lt;p>Serverless computing has emerged as a widely-used paradigm for running services in the cloud. In serverless, developers organize their applications as a set of functions, which are invoked on-demand in response to events, such as an HTTP request. To avoid long start-up delays of launching a new function instance, cloud providers tend to keep recently-triggered instances idle (or warm) for some time after the most recent invocation in anticipation of future invocations. Thus, at any given moment on a server, there may be thousands of warm instances of various functions whose executions are interleaved in time based on incoming invocations.&lt;/p>
&lt;p>This paper observes that (1) there is a high degree of interleaving among warm instances on a given server; (2) the individual warm functions are invoked relatively infrequently, often at the granularity of seconds or minutes; and (3) many function invocations complete within a few milliseconds. Interleaved execution of rarely invoked functions on a server leads to thrashing of each function&amp;rsquo;s microarchitectural state between invocations. Meanwhile, the short execution time of a function impedes amortization of the warm-up latency of the cache hierarchy, causing a 31-114% increase in CPI compared to execution with warm microarchitectural state. We identify on-chip misses for instructions as a major contributor to the performance loss. In response we propose Jukebox, a record-and-replay instruction prefetcher specifically designed for reducing the start-up latency of warm function instances. Jukebox requires just 32KB of metadata per function instance and boosts performance by an average of 18.7% for a wide range of functions, which translates into a corresponding throughput improvement.&lt;/p></description></item></channel></rss>